{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ad43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the saved CSV file\n",
    "data = pd.read_csv('processed_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f2a1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_Start</th>\n",
       "      <th>Y_Start</th>\n",
       "      <th>X_End</th>\n",
       "      <th>Y_end</th>\n",
       "      <th>Z</th>\n",
       "      <th>meanLineIntensity</th>\n",
       "      <th>varLineIntensity</th>\n",
       "      <th>meanNumPixels15</th>\n",
       "      <th>varNumPixels15</th>\n",
       "      <th>meanNumPixels30</th>\n",
       "      <th>...</th>\n",
       "      <th>varNumPixels60</th>\n",
       "      <th>Median_meltLength</th>\n",
       "      <th>varMeltLength</th>\n",
       "      <th>Overhang</th>\n",
       "      <th>Wall</th>\n",
       "      <th>Power</th>\n",
       "      <th>d_edge</th>\n",
       "      <th>d_over</th>\n",
       "      <th>d_col</th>\n",
       "      <th>d_over_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.828807e+06</td>\n",
       "      <td>5.902107e+09</td>\n",
       "      <td>4884.797753</td>\n",
       "      <td>9.249608e+05</td>\n",
       "      <td>1195.752809</td>\n",
       "      <td>...</td>\n",
       "      <td>18681.207099</td>\n",
       "      <td>42</td>\n",
       "      <td>47.422625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.914433e+06</td>\n",
       "      <td>5.807522e+09</td>\n",
       "      <td>6706.269663</td>\n",
       "      <td>1.771474e+06</td>\n",
       "      <td>1619.438202</td>\n",
       "      <td>...</td>\n",
       "      <td>26267.830695</td>\n",
       "      <td>41</td>\n",
       "      <td>52.967314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.910230e+06</td>\n",
       "      <td>4.765438e+09</td>\n",
       "      <td>6109.460674</td>\n",
       "      <td>7.615990e+05</td>\n",
       "      <td>1612.179775</td>\n",
       "      <td>...</td>\n",
       "      <td>39714.737487</td>\n",
       "      <td>43</td>\n",
       "      <td>77.342952</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.905874e+06</td>\n",
       "      <td>7.406520e+09</td>\n",
       "      <td>5864.943820</td>\n",
       "      <td>1.277515e+06</td>\n",
       "      <td>1534.775281</td>\n",
       "      <td>...</td>\n",
       "      <td>22738.784729</td>\n",
       "      <td>43</td>\n",
       "      <td>42.477017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.844209e+06</td>\n",
       "      <td>3.619133e+09</td>\n",
       "      <td>5074.494382</td>\n",
       "      <td>3.498485e+05</td>\n",
       "      <td>1490.022472</td>\n",
       "      <td>...</td>\n",
       "      <td>25185.487487</td>\n",
       "      <td>41</td>\n",
       "      <td>25.244893</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.859401e+06</td>\n",
       "      <td>7.614869e+09</td>\n",
       "      <td>5572.820225</td>\n",
       "      <td>2.422673e+06</td>\n",
       "      <td>1103.820225</td>\n",
       "      <td>...</td>\n",
       "      <td>7935.262002</td>\n",
       "      <td>37</td>\n",
       "      <td>40.245403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>0</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.933627e+06</td>\n",
       "      <td>6.933997e+09</td>\n",
       "      <td>6258.314607</td>\n",
       "      <td>1.999094e+06</td>\n",
       "      <td>1383.382022</td>\n",
       "      <td>...</td>\n",
       "      <td>20979.463228</td>\n",
       "      <td>40</td>\n",
       "      <td>133.660368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>0</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.880439e+06</td>\n",
       "      <td>7.908295e+09</td>\n",
       "      <td>5551.235955</td>\n",
       "      <td>2.410604e+06</td>\n",
       "      <td>1299.382022</td>\n",
       "      <td>...</td>\n",
       "      <td>18303.926200</td>\n",
       "      <td>40</td>\n",
       "      <td>60.948927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>0</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.889719e+06</td>\n",
       "      <td>1.324928e+10</td>\n",
       "      <td>5593.842697</td>\n",
       "      <td>2.631757e+06</td>\n",
       "      <td>1325.471910</td>\n",
       "      <td>...</td>\n",
       "      <td>15455.863126</td>\n",
       "      <td>41</td>\n",
       "      <td>60.363126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>0</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.925649e+06</td>\n",
       "      <td>8.194131e+09</td>\n",
       "      <td>6093.943820</td>\n",
       "      <td>2.612716e+06</td>\n",
       "      <td>1494.550562</td>\n",
       "      <td>...</td>\n",
       "      <td>18409.146578</td>\n",
       "      <td>40</td>\n",
       "      <td>63.499489</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1812 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      X_Start  Y_Start  X_End  Y_end     Z  meanLineIntensity  \\\n",
       "0           0     0.05    1.5   0.05  0.50       1.828807e+06   \n",
       "1           0     0.10    1.5   0.10  0.50       1.914433e+06   \n",
       "2           0     0.15    1.5   0.15  0.50       1.910230e+06   \n",
       "3           0     0.20    1.5   0.20  0.50       1.905874e+06   \n",
       "4           0     0.25    1.5   0.25  0.50       1.844209e+06   \n",
       "...       ...      ...    ...    ...   ...                ...   \n",
       "1807        0     3.15    1.5   3.15  0.45       1.859401e+06   \n",
       "1808        0     3.20    1.5   3.20  0.45       1.933627e+06   \n",
       "1809        0     3.25    1.5   3.25  0.45       1.880439e+06   \n",
       "1810        0     3.30    1.5   3.30  0.45       1.889719e+06   \n",
       "1811        0     3.35    1.5   3.35  0.45       1.925649e+06   \n",
       "\n",
       "      varLineIntensity  meanNumPixels15  varNumPixels15  meanNumPixels30  ...  \\\n",
       "0         5.902107e+09      4884.797753    9.249608e+05      1195.752809  ...   \n",
       "1         5.807522e+09      6706.269663    1.771474e+06      1619.438202  ...   \n",
       "2         4.765438e+09      6109.460674    7.615990e+05      1612.179775  ...   \n",
       "3         7.406520e+09      5864.943820    1.277515e+06      1534.775281  ...   \n",
       "4         3.619133e+09      5074.494382    3.498485e+05      1490.022472  ...   \n",
       "...                ...              ...             ...              ...  ...   \n",
       "1807      7.614869e+09      5572.820225    2.422673e+06      1103.820225  ...   \n",
       "1808      6.933997e+09      6258.314607    1.999094e+06      1383.382022  ...   \n",
       "1809      7.908295e+09      5551.235955    2.410604e+06      1299.382022  ...   \n",
       "1810      1.324928e+10      5593.842697    2.631757e+06      1325.471910  ...   \n",
       "1811      8.194131e+09      6093.943820    2.612716e+06      1494.550562  ...   \n",
       "\n",
       "      varNumPixels60  Median_meltLength  varMeltLength  Overhang  Wall  Power  \\\n",
       "0       18681.207099                 42      47.422625         0     0    180   \n",
       "1       26267.830695                 41      52.967314         0     0    180   \n",
       "2       39714.737487                 43      77.342952         0     0    180   \n",
       "3       22738.784729                 43      42.477017         0     0    180   \n",
       "4       25185.487487                 41      25.244893         0     0    180   \n",
       "...              ...                ...            ...       ...   ...    ...   \n",
       "1807     7935.262002                 37      40.245403         0     0    180   \n",
       "1808    20979.463228                 40     133.660368         0     0    180   \n",
       "1809    18303.926200                 40      60.948927         0     0    180   \n",
       "1810    15455.863126                 41      60.363126         0     0    180   \n",
       "1811    18409.146578                 40      63.499489         0     0    180   \n",
       "\n",
       "      d_edge  d_over  d_col  d_over_z  \n",
       "0       0.05     3.3    0.0      1.65  \n",
       "1       0.05     3.3    0.0      1.65  \n",
       "2       0.10     3.3    0.0      1.65  \n",
       "3       0.15     3.3    0.0      1.65  \n",
       "4       0.20     3.3    0.0      1.65  \n",
       "...      ...     ...    ...       ...  \n",
       "1807    0.20     3.3    0.0      1.65  \n",
       "1808    0.15     3.3    0.0      1.65  \n",
       "1809    0.10     3.3    0.0      1.65  \n",
       "1810    0.05     3.3    0.0      1.65  \n",
       "1811    0.05     3.3    0.0      1.65  \n",
       "\n",
       "[1812 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49311fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define target and input variables\n",
    "target_cols = ['Median_meltLength', 'meanNumPixels15']\n",
    "input_cols = ['Power', 'Z', 'Y_Start', 'Wall', 'd_edge', 'd_over', 'd_col', 'd_over_z']\n",
    "\n",
    "# Extract features and targets\n",
    "X = data[input_cols].values\n",
    "y = data[target_cols].values\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Function to prepare data loaders for a specific fold\n",
    "def prepare_fold_data(X, y, train_idx, val_idx, batch_size=32):\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X[train_idx])\n",
    "    X_val = scaler.transform(X[val_idx])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(X_train, y[train_idx])\n",
    "    val_dataset = CustomDataset(X_val, y[val_idx])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, scaler\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Store fold indices for later use\n",
    "fold_indices = []\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    fold_indices.append((train_idx, val_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af94480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import combinations_with_replacement, combinations\n",
    "\n",
    "class PolynomialRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=8, output_dim=2, degree=3):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.degree = degree\n",
    "        \n",
    "        # Calculate number of features after polynomial expansion\n",
    "        # Original features + squares + double interactions + triple interactions\n",
    "        n_original = input_dim\n",
    "        n_squares = input_dim  # x1^2, x2^2, etc.\n",
    "        n_double_interactions = len(list(combinations_with_replacement(range(input_dim), 2))) - input_dim  # x1*x2, x1*x3, etc.\n",
    "        n_triple_interactions = len(list(combinations(range(input_dim), 3)))  # x1*x2*x3, etc.\n",
    "        \n",
    "        self.expanded_dim = n_original + n_squares + n_double_interactions + n_triple_interactions\n",
    "        \n",
    "        print(f\"Feature space dimensionality:\")\n",
    "        print(f\"Original features: {n_original}\")\n",
    "        print(f\"Squared terms: {n_squares}\")\n",
    "        print(f\"Double interactions: {n_double_interactions}\")\n",
    "        print(f\"Triple interactions: {n_triple_interactions}\")\n",
    "        print(f\"Total features: {self.expanded_dim}\")\n",
    "        \n",
    "        # Linear layer for the expanded features\n",
    "        self.linear = nn.Linear(self.expanded_dim, output_dim)\n",
    "        \n",
    "    def compute_polynomial_features(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Original features\n",
    "        features = [x]\n",
    "        \n",
    "        # Squared terms\n",
    "        squares = x ** 2\n",
    "        features.append(squares)\n",
    "        \n",
    "        # Double interaction terms\n",
    "        double_interactions = []\n",
    "        for i in range(self.input_dim):\n",
    "            for j in range(i+1, self.input_dim):\n",
    "                interaction = x[:, i] * x[:, j]\n",
    "                double_interactions.append(interaction.unsqueeze(1))\n",
    "        \n",
    "        if double_interactions:\n",
    "            double_interactions_tensor = torch.cat(double_interactions, dim=1)\n",
    "            features.append(double_interactions_tensor)\n",
    "        \n",
    "        # Triple interaction terms\n",
    "        triple_interactions = []\n",
    "        for i, j, k in combinations(range(self.input_dim), 3):\n",
    "            interaction = x[:, i] * x[:, j] * x[:, k]\n",
    "            triple_interactions.append(interaction.unsqueeze(1))\n",
    "        \n",
    "        if triple_interactions:\n",
    "            triple_interactions_tensor = torch.cat(triple_interactions, dim=1)\n",
    "            features.append(triple_interactions_tensor)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        return torch.cat(features, dim=1)\n",
    "    \n",
    "    def get_feature_names(self, input_names=None):\n",
    "        \"\"\"\n",
    "        Returns the names of all features after polynomial expansion\n",
    "        \"\"\"\n",
    "        if input_names is None:\n",
    "            input_names = [f'x{i}' for i in range(self.input_dim)]\n",
    "            \n",
    "        feature_names = []\n",
    "        \n",
    "        # Original features\n",
    "        feature_names.extend(input_names)\n",
    "        \n",
    "        # Squared terms\n",
    "        feature_names.extend([f'{x}²' for x in input_names])\n",
    "        \n",
    "        # Double interaction terms\n",
    "        for i in range(self.input_dim):\n",
    "            for j in range(i+1, self.input_dim):\n",
    "                feature_names.append(f'{input_names[i]}*{input_names[j]}')\n",
    "        \n",
    "        # Triple interaction terms\n",
    "        for i, j, k in combinations(range(self.input_dim), 3):\n",
    "            feature_names.append(f'{input_names[i]}*{input_names[j]}*{input_names[k]}')\n",
    "        \n",
    "        return feature_names\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute polynomial features\n",
    "        x_poly = self.compute_polynomial_features(x)\n",
    "        # Apply linear regression on expanded features\n",
    "        return self.linear(x_poly)\n",
    "    \n",
    "    def get_feature_importance(self, input_names=None):\n",
    "        \"\"\"\n",
    "        Returns the importance (weight magnitude) of each feature\n",
    "        \"\"\"\n",
    "        if input_names is None:\n",
    "            input_names = [f'x{i}' for i in range(self.input_dim)]\n",
    "\n",
    "        weights = self.linear.weight.detach().cpu()  # Shape: [output_dim, expanded_dim]\n",
    "        importance = torch.abs(weights).mean(dim=0)  # Average across output dimensions\n",
    "\n",
    "        feature_names = self.get_feature_names(input_names)  # Use the provided input names\n",
    "        importance_dict = {name: float(imp) for name, imp in zip(feature_names, importance)}\n",
    "\n",
    "        # Sort by importance\n",
    "        importance_dict = dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        return importance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a3d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def plot_feature_importance(importance_across_folds, save_dir='./plots'):\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Calculate mean and std of importance across folds\n",
    "    mean_importance = {}\n",
    "    std_importance = {}\n",
    "    \n",
    "    all_features = importance_across_folds[0].keys()\n",
    "    \n",
    "    for feature in all_features:\n",
    "        values = [fold_imp[feature] for fold_imp in importance_across_folds]\n",
    "        mean_importance[feature] = np.mean(values)\n",
    "        std_importance[feature] = np.std(values)\n",
    "    \n",
    "    # Sort by mean importance\n",
    "    sorted_features = sorted(mean_importance.keys(), \n",
    "                           key=lambda x: mean_importance[x], \n",
    "                           reverse=True)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    y_pos = np.arange(len(sorted_features))\n",
    "    \n",
    "    # Plot bars with error bars\n",
    "    plt.barh(y_pos, \n",
    "            [mean_importance[feat] for feat in sorted_features],\n",
    "            xerr=[std_importance[feat] for feat in sorted_features],\n",
    "            capsize=5)\n",
    "    \n",
    "    plt.yticks(y_pos, sorted_features)\n",
    "    plt.xlabel('Feature Importance (Mean Absolute Weight)')\n",
    "    plt.title('Feature Importance Across Folds')\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(f'{save_dir}/feature_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_correlation_matrix(feature_matrix, feature_names, save_dir='./plots'):\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(feature_matrix.T)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(corr_matrix, \n",
    "                xticklabels=feature_names, \n",
    "                yticklabels=feature_names,\n",
    "                cmap='RdBu_r', \n",
    "                center=0,\n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                square=True)\n",
    "    \n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_dir}/feature_correlation.png')\n",
    "    plt.close()\n",
    "\n",
    "def run_cross_validation(model, fold_indices, X, y, num_epochs=100):\n",
    "    all_metrics = []\n",
    "    importance_across_folds = []\n",
    "    feature_names = model.get_feature_names(['Power', 'Z', 'Y_Start', 'Wall', 'd_edge', 'd_over', 'd_col', 'd_over_z'])\n",
    "    \n",
    "    print(\"\\nInitial feature space analysis:\")\n",
    "    print(f\"Number of features after expansion: {model.expanded_dim}\")\n",
    "    print(\"\\nFeature types:\")\n",
    "    for name in feature_names:\n",
    "        print(f\"- {name}\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(fold_indices):\n",
    "        print(f'\\nTraining fold {fold+1}')\n",
    "        \n",
    "        # Prepare data for this fold\n",
    "        train_loader, val_loader, scaler = prepare_fold_data(X, y, train_idx, val_idx, batch_size=len(train_idx))\n",
    "        \n",
    "        # Initialize new model for this fold\n",
    "        fold_model = PolynomialRegressor(input_dim=8, output_dim=2)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Initialize L-BFGS optimizer\n",
    "        optimizer = torch.optim.LBFGS(fold_model.parameters(), \n",
    "                                    lr=1.0,\n",
    "                                    max_iter=20,\n",
    "                                    history_size=10,\n",
    "                                    line_search_fn='strong_wolfe')\n",
    "        \n",
    "        # Train the model\n",
    "        fold_model, history = train_model(\n",
    "            model=fold_model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            num_epochs=num_epochs,\n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plot_training_history(history, fold)\n",
    "        \n",
    "        # Calculate and store metrics\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        metrics = calculate_metrics(fold_model, val_loader, device)\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        feature_names = ['Power', 'Z', 'Y_Start', 'Wall', 'd_edge', 'd_over', 'd_col', 'd_over_z']\n",
    "\n",
    "        \n",
    "        # Get feature importance for this fold\n",
    "        fold_importance = fold_model.get_feature_importance(input_names=feature_names)\n",
    "        importance_across_folds.append(fold_importance)\n",
    "        \n",
    "        # Print metrics for this fold\n",
    "        print(f'\\nMetrics for Fold {fold+1}:')\n",
    "        for target, target_metrics in metrics.items():\n",
    "            print(f'\\n{target}:')\n",
    "            print(f'R² Score: {target_metrics[\"r2\"]:.4f}')\n",
    "            print(f'RMSE: {target_metrics[\"rmse\"]:.4f}')\n",
    "            print(f'MAE: {target_metrics[\"mae\"]:.4f}')\n",
    "        \n",
    "        # Print top 5 most important features for this fold\n",
    "        print(f'\\nTop 5 most important features for fold {fold+1}:')\n",
    "        for idx, (feature, importance) in enumerate(list(fold_importance.items())[:5]):\n",
    "            print(f\"{idx+1}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "        # Generate feature matrix for correlation analysis\n",
    "        if fold == 0:  # Do this only for first fold to avoid redundancy\n",
    "            # Get polynomial features for the training data\n",
    "            X_train = X[train_idx]\n",
    "            X_train_tensor = torch.FloatTensor(X_train)\n",
    "            poly_features = fold_model.compute_polynomial_features(X_train_tensor).detach().numpy()\n",
    "            \n",
    "            # Plot correlation matrix\n",
    "            plot_correlation_matrix(poly_features, feature_names)\n",
    "    \n",
    "    # Plot overall feature importance\n",
    "    plot_feature_importance(importance_across_folds)\n",
    "    \n",
    "    # Print average metrics across all folds\n",
    "    print('\\nAverage metrics across all folds:')\n",
    "    for target in ['Median_meltLength', 'meanNumPixels15']:\n",
    "        avg_r2 = np.mean([metrics[target]['r2'] for metrics in all_metrics])\n",
    "        avg_rmse = np.mean([metrics[target]['rmse'] for metrics in all_metrics])\n",
    "        avg_mae = np.mean([metrics[target]['mae'] for metrics in all_metrics])\n",
    "        \n",
    "        print(f'\\n{target}:')\n",
    "        print(f'Average R² Score: {avg_r2:.4f} ± {np.std([metrics[target][\"r2\"] for metrics in all_metrics]):.4f}')\n",
    "        print(f'Average RMSE: {avg_rmse:.4f} ± {np.std([metrics[target][\"rmse\"] for metrics in all_metrics]):.4f}')\n",
    "        print(f'Average MAE: {avg_mae:.4f} ± {np.std([metrics[target][\"mae\"] for metrics in all_metrics]):.4f}')\n",
    "    \n",
    "    # Print average feature importance across folds\n",
    "    print('\\nAverage feature importance across folds:')\n",
    "    avg_importance = {}\n",
    "    for feature in importance_across_folds[0].keys():\n",
    "        values = [fold_imp[feature] for fold_imp in importance_across_folds]\n",
    "        avg_importance[feature] = np.mean(values)\n",
    "    \n",
    "    # Sort and print top 10 features\n",
    "    sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    print('\\nTop 10 most important features (averaged across folds):')\n",
    "    for feature, importance in sorted_features[:10]:\n",
    "        values = [fold_imp[feature] for fold_imp in importance_across_folds]\n",
    "        print(f\"{feature}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
    "    \n",
    "    return all_metrics, importance_across_folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74188925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from pathlib import Path\n",
    "\n",
    "def closure(model, X_batch, y_batch, criterion, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_batch)\n",
    "    loss = criterion(output, y_batch)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, \n",
    "                early_stopping_patience=1000, fold=0, save_dir='./models'):\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    training_history = {'train_loss': [], 'val_loss': []}\n",
    "    best_model_path = f'{save_dir}/best_model_fold_{fold+1}.pt'\n",
    "    \n",
    "    # Convert training data to single batch for L-BFGS\n",
    "    X_train_all = []\n",
    "    y_train_all = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_train_all.append(X_batch)\n",
    "        y_train_all.append(y_batch)\n",
    "    X_train_all = torch.cat(X_train_all).to(device)\n",
    "    y_train_all = torch.cat(y_train_all).to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        \n",
    "        # Define closure for L-BFGS\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_train_all)\n",
    "            loss = criterion(outputs, y_train_all)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        # Perform optimization step\n",
    "        loss = optimizer.step(closure)\n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                val_loss += criterion(outputs, y_batch).item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = train_loss  # For L-BFGS, this is already the average\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Store losses\n",
    "        training_history['train_loss'].append(avg_train_loss)\n",
    "        training_history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "            }, best_model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Load best model before returning\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f0f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            y_true.append(y_batch.numpy())\n",
    "            y_pred.append(outputs.cpu().numpy())\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    # Calculate metrics for each target variable\n",
    "    metrics = {}\n",
    "    for i, target_name in enumerate(['Median_meltLength', 'meanNumPixels15']):\n",
    "        metrics[target_name] = {\n",
    "            'r2': r2_score(y_true[:, i], y_pred[:, i]),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i])),\n",
    "            'mae': mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_training_history(history, fold, save_dir='./plots'):\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Loss Evolution - Fold {fold+1}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{save_dir}/loss_evolution_fold_{fold+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dce8787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature space dimensionality:\n",
      "Original features: 8\n",
      "Squared terms: 8\n",
      "Double interactions: 28\n",
      "Triple interactions: 56\n",
      "Total features: 100\n",
      "\n",
      "Initial feature space analysis:\n",
      "Number of features after expansion: 100\n",
      "\n",
      "Feature types:\n",
      "- Power\n",
      "- Z\n",
      "- Y_Start\n",
      "- Wall\n",
      "- d_edge\n",
      "- d_over\n",
      "- d_col\n",
      "- d_over_z\n",
      "- Power²\n",
      "- Z²\n",
      "- Y_Start²\n",
      "- Wall²\n",
      "- d_edge²\n",
      "- d_over²\n",
      "- d_col²\n",
      "- d_over_z²\n",
      "- Power*Z\n",
      "- Power*Y_Start\n",
      "- Power*Wall\n",
      "- Power*d_edge\n",
      "- Power*d_over\n",
      "- Power*d_col\n",
      "- Power*d_over_z\n",
      "- Z*Y_Start\n",
      "- Z*Wall\n",
      "- Z*d_edge\n",
      "- Z*d_over\n",
      "- Z*d_col\n",
      "- Z*d_over_z\n",
      "- Y_Start*Wall\n",
      "- Y_Start*d_edge\n",
      "- Y_Start*d_over\n",
      "- Y_Start*d_col\n",
      "- Y_Start*d_over_z\n",
      "- Wall*d_edge\n",
      "- Wall*d_over\n",
      "- Wall*d_col\n",
      "- Wall*d_over_z\n",
      "- d_edge*d_over\n",
      "- d_edge*d_col\n",
      "- d_edge*d_over_z\n",
      "- d_over*d_col\n",
      "- d_over*d_over_z\n",
      "- d_col*d_over_z\n",
      "- Power*Z*Y_Start\n",
      "- Power*Z*Wall\n",
      "- Power*Z*d_edge\n",
      "- Power*Z*d_over\n",
      "- Power*Z*d_col\n",
      "- Power*Z*d_over_z\n",
      "- Power*Y_Start*Wall\n",
      "- Power*Y_Start*d_edge\n",
      "- Power*Y_Start*d_over\n",
      "- Power*Y_Start*d_col\n",
      "- Power*Y_Start*d_over_z\n",
      "- Power*Wall*d_edge\n",
      "- Power*Wall*d_over\n",
      "- Power*Wall*d_col\n",
      "- Power*Wall*d_over_z\n",
      "- Power*d_edge*d_over\n",
      "- Power*d_edge*d_col\n",
      "- Power*d_edge*d_over_z\n",
      "- Power*d_over*d_col\n",
      "- Power*d_over*d_over_z\n",
      "- Power*d_col*d_over_z\n",
      "- Z*Y_Start*Wall\n",
      "- Z*Y_Start*d_edge\n",
      "- Z*Y_Start*d_over\n",
      "- Z*Y_Start*d_col\n",
      "- Z*Y_Start*d_over_z\n",
      "- Z*Wall*d_edge\n",
      "- Z*Wall*d_over\n",
      "- Z*Wall*d_col\n",
      "- Z*Wall*d_over_z\n",
      "- Z*d_edge*d_over\n",
      "- Z*d_edge*d_col\n",
      "- Z*d_edge*d_over_z\n",
      "- Z*d_over*d_col\n",
      "- Z*d_over*d_over_z\n",
      "- Z*d_col*d_over_z\n",
      "- Y_Start*Wall*d_edge\n",
      "- Y_Start*Wall*d_over\n",
      "- Y_Start*Wall*d_col\n",
      "- Y_Start*Wall*d_over_z\n",
      "- Y_Start*d_edge*d_over\n",
      "- Y_Start*d_edge*d_col\n",
      "- Y_Start*d_edge*d_over_z\n",
      "- Y_Start*d_over*d_col\n",
      "- Y_Start*d_over*d_over_z\n",
      "- Y_Start*d_col*d_over_z\n",
      "- Wall*d_edge*d_over\n",
      "- Wall*d_edge*d_col\n",
      "- Wall*d_edge*d_over_z\n",
      "- Wall*d_over*d_col\n",
      "- Wall*d_over*d_over_z\n",
      "- Wall*d_col*d_over_z\n",
      "- d_edge*d_over*d_col\n",
      "- d_edge*d_over*d_over_z\n",
      "- d_edge*d_col*d_over_z\n",
      "- d_over*d_col*d_over_z\n",
      "\n",
      "Training fold 1\n",
      "Feature space dimensionality:\n",
      "Original features: 8\n",
      "Squared terms: 8\n",
      "Double interactions: 28\n",
      "Triple interactions: 56\n",
      "Total features: 100\n",
      "Epoch [10/1000], Train Loss: 239322.0312, Val Loss: 242637.0938\n",
      "Epoch [20/1000], Train Loss: 235819.0625, Val Loss: 246075.9688\n",
      "Epoch [30/1000], Train Loss: 235359.5312, Val Loss: 247976.5469\n",
      "Epoch [40/1000], Train Loss: 235233.2031, Val Loss: 248271.3906\n",
      "Epoch [50/1000], Train Loss: 235174.9531, Val Loss: 248016.8906\n",
      "Epoch [60/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [70/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [80/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [90/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [100/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [110/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [120/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [130/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [140/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [150/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [160/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [170/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [180/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [190/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [200/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [210/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [220/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [230/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [240/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [250/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [260/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [270/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [280/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [290/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [300/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [310/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [320/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [330/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [340/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [350/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [360/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [370/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [380/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [390/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [400/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [410/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [420/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [430/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [440/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [450/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [460/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [470/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [480/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [490/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [500/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [510/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [520/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [530/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [540/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [550/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [560/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [570/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [580/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [590/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [600/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [610/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [620/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [630/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [640/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [650/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [660/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [670/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [680/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [690/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [700/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [710/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [720/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [730/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [740/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [750/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [760/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [770/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [780/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [790/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [800/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [810/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [820/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [830/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [840/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [850/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [860/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [870/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [880/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [890/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [900/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [910/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [920/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [930/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [940/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [950/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [960/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [970/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [980/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [990/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "Epoch [1000/1000], Train Loss: 235165.9219, Val Loss: 248091.0469\n",
      "\n",
      "Metrics for Fold 1:\n",
      "\n",
      "Median_meltLength:\n",
      "R² Score: -2.3744\n",
      "RMSE: 8.0992\n",
      "MAE: 4.5114\n",
      "\n",
      "meanNumPixels15:\n",
      "R² Score: 0.4049\n",
      "RMSE: 696.5692\n",
      "MAE: 551.0764\n",
      "\n",
      "Top 5 most important features for fold 1:\n",
      "1. d_over_z: 889.0762\n",
      "2. Z*d_col*d_over_z: 713.3776\n",
      "3. d_col: 656.7336\n",
      "4. Z*d_over*d_col: 513.1658\n",
      "5. d_col*d_over_z: 379.4509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juls/Adaptive-Filter-for-Acoustic-Signal-Segmentation/myenv/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:2999: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/juls/Adaptive-Filter-for-Acoustic-Signal-Segmentation/myenv/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3000: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 2\n",
      "Feature space dimensionality:\n",
      "Original features: 8\n",
      "Squared terms: 8\n",
      "Double interactions: 28\n",
      "Triple interactions: 56\n",
      "Total features: 100\n",
      "Epoch [10/1000], Train Loss: 236972.1562, Val Loss: 254101.2031\n",
      "Epoch [20/1000], Train Loss: 233090.3594, Val Loss: 258402.4688\n",
      "Epoch [30/1000], Train Loss: 232777.3125, Val Loss: 258373.2031\n",
      "Epoch [40/1000], Train Loss: 232693.0625, Val Loss: 256851.1406\n",
      "Epoch [50/1000], Train Loss: 232578.2031, Val Loss: 256955.5781\n",
      "Epoch [60/1000], Train Loss: 232554.1250, Val Loss: 257122.8750\n",
      "Epoch [70/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [80/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [90/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [100/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [110/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [120/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [130/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [140/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [150/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [160/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [170/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [180/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [190/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [200/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [210/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [220/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [230/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [240/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [250/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [260/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [270/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [280/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [290/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [300/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [310/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [320/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [330/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [340/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [350/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [360/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [370/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [380/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [390/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [400/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [410/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [420/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [430/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [440/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [450/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [460/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [470/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [480/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [490/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [500/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [510/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [520/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [530/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [540/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [550/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [560/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [570/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [580/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [590/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [600/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [610/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [620/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [630/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [640/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [650/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [660/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [670/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [680/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [690/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [700/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [710/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [720/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [730/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [740/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [750/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [760/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [770/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [780/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [790/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [800/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [810/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [820/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [830/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [840/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [850/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [860/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [870/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [880/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [890/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [900/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [910/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [920/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [930/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [940/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [950/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [960/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [970/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [980/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [990/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "Epoch [1000/1000], Train Loss: 232553.5156, Val Loss: 257072.2031\n",
      "\n",
      "Metrics for Fold 2:\n",
      "\n",
      "Median_meltLength:\n",
      "R² Score: 0.2686\n",
      "RMSE: 3.5025\n",
      "MAE: 2.5449\n",
      "\n",
      "meanNumPixels15:\n",
      "R² Score: 0.3538\n",
      "RMSE: 711.8799\n",
      "MAE: 564.3442\n",
      "\n",
      "Top 5 most important features for fold 2:\n",
      "1. d_over_z: 824.6803\n",
      "2. d_col: 636.3683\n",
      "3. Z*d_col*d_over_z: 589.6943\n",
      "4. Z*d_over*d_col: 464.1091\n",
      "5. d_edge: 384.0181\n",
      "\n",
      "Training fold 3\n",
      "Feature space dimensionality:\n",
      "Original features: 8\n",
      "Squared terms: 8\n",
      "Double interactions: 28\n",
      "Triple interactions: 56\n",
      "Total features: 100\n",
      "Epoch [10/1000], Train Loss: 239048.8750, Val Loss: 248915.9375\n",
      "Epoch [20/1000], Train Loss: 236923.6875, Val Loss: 239489.0625\n",
      "Epoch [30/1000], Train Loss: 236640.0469, Val Loss: 241196.5625\n",
      "Epoch [40/1000], Train Loss: 236465.7031, Val Loss: 241317.5312\n",
      "Epoch [50/1000], Train Loss: 236354.7812, Val Loss: 240854.5469\n",
      "Epoch [60/1000], Train Loss: 236319.9219, Val Loss: 240927.5625\n",
      "Epoch [70/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [80/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [90/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [100/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [110/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [120/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [130/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [140/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [150/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [160/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [170/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [190/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [200/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [210/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [220/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [230/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [240/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [250/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [260/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [270/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [280/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [290/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [300/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [310/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [320/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [330/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [340/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [350/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [360/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [370/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [380/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [390/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [400/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [410/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [420/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [430/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [440/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [450/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [460/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [470/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [480/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [490/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [500/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [510/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [520/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [530/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [540/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [550/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [560/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [570/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [580/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [590/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [600/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [610/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [620/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [630/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [640/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [650/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [660/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [670/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [680/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [690/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [700/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [710/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [720/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [730/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [740/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [750/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [760/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [770/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [780/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [790/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [800/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [810/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [820/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [830/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [840/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [850/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [860/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [870/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [880/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [890/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [900/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [910/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [920/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [930/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [940/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [950/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [960/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [970/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [980/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [990/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "Epoch [1000/1000], Train Loss: 236308.3750, Val Loss: 241039.5625\n",
      "\n",
      "Metrics for Fold 3:\n",
      "\n",
      "Median_meltLength:\n",
      "R² Score: 0.4104\n",
      "RMSE: 3.6287\n",
      "MAE: 2.7707\n",
      "\n",
      "meanNumPixels15:\n",
      "R² Score: 0.4550\n",
      "RMSE: 692.0729\n",
      "MAE: 550.0991\n",
      "\n",
      "Top 5 most important features for fold 3:\n",
      "1. d_over_z: 867.3974\n",
      "2. d_col: 812.3239\n",
      "3. Z*d_over*d_col: 692.5014\n",
      "4. Z*d_col*d_over_z: 578.0751\n",
      "5. Y_Start*Wall: 491.9736\n",
      "\n",
      "Training fold 4\n",
      "Feature space dimensionality:\n",
      "Original features: 8\n",
      "Squared terms: 8\n",
      "Double interactions: 28\n",
      "Triple interactions: 56\n",
      "Total features: 100\n",
      "Epoch [10/1000], Train Loss: 234267.7188, Val Loss: 270360.9688\n",
      "Epoch [20/1000], Train Loss: 230818.0781, Val Loss: 268068.4688\n",
      "Epoch [30/1000], Train Loss: 230280.7969, Val Loss: 268743.0938\n",
      "Epoch [40/1000], Train Loss: 230149.2188, Val Loss: 269932.5625\n",
      "Epoch [50/1000], Train Loss: 230059.4531, Val Loss: 270265.8750\n",
      "Epoch [60/1000], Train Loss: 230024.4219, Val Loss: 269401.7188\n",
      "Epoch [70/1000], Train Loss: 230007.7344, Val Loss: 269298.5625\n",
      "Epoch [80/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [90/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [100/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [110/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [120/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [130/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [140/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [150/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [160/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [170/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [180/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [190/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [200/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [210/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [220/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [230/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [240/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [250/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [260/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [270/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [280/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [290/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [300/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [310/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [320/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [330/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [340/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [350/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [360/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [370/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [380/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [390/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [400/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [410/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [420/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [430/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [440/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [450/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [460/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [470/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [480/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [490/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [500/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [510/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [520/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [530/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [540/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [550/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [560/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [570/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [580/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [590/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [600/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [610/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [620/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [630/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [640/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [650/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [660/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [670/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [680/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [690/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [700/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [710/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [720/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [730/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [740/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [750/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [760/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [770/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [780/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [790/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [800/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [810/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [820/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [830/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [840/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [850/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [860/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [870/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [880/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [890/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [900/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [910/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [920/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [930/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [940/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [950/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [960/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [970/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [980/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [990/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "Epoch [1000/1000], Train Loss: 230006.6094, Val Loss: 269306.5625\n",
      "\n",
      "Metrics for Fold 4:\n",
      "\n",
      "Median_meltLength:\n",
      "R² Score: 0.2207\n",
      "RMSE: 4.2885\n",
      "MAE: 3.0586\n",
      "\n",
      "meanNumPixels15:\n",
      "R² Score: 0.4071\n",
      "RMSE: 718.0037\n",
      "MAE: 574.0906\n",
      "\n",
      "Top 5 most important features for fold 4:\n",
      "1. d_over_z: 816.0956\n",
      "2. d_col: 645.5450\n",
      "3. d_edge: 467.3935\n",
      "4. Z*d_col*d_over_z: 387.7245\n",
      "5. Z*d_edge*d_over: 318.8975\n",
      "\n",
      "Training fold 5\n",
      "Feature space dimensionality:\n",
      "Original features: 8\n",
      "Squared terms: 8\n",
      "Double interactions: 28\n",
      "Triple interactions: 56\n",
      "Total features: 100\n",
      "Epoch [10/1000], Train Loss: 236416.5000, Val Loss: 264907.2500\n",
      "Epoch [20/1000], Train Loss: 234361.8750, Val Loss: 259438.4219\n",
      "Epoch [30/1000], Train Loss: 233934.0000, Val Loss: 255986.6875\n",
      "Epoch [40/1000], Train Loss: 233741.6406, Val Loss: 256072.5000\n",
      "Epoch [50/1000], Train Loss: 233640.3906, Val Loss: 257023.0156\n",
      "Epoch [60/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [70/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [80/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [90/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [100/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [110/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [120/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [130/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [140/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [150/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [160/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [170/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [180/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [190/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [200/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [210/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [220/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [230/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [240/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [250/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [260/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [270/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [280/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [290/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [300/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [310/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [320/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [330/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [340/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [350/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [360/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [370/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [380/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [390/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [400/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [410/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [420/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [430/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [440/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [450/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [460/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [470/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [480/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [490/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [500/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [510/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [520/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [530/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [540/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [550/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [560/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [570/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [580/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [590/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [600/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [610/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [620/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [630/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [640/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [650/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [660/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [670/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [680/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [690/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [700/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [710/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [720/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [730/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [740/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [750/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [760/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [770/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [780/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [790/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [800/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [810/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [820/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [830/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [840/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [850/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [860/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [870/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [880/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [890/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [900/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [910/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [920/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [930/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [940/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [950/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [960/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [970/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [980/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [990/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "Epoch [1000/1000], Train Loss: 233628.4688, Val Loss: 257300.7656\n",
      "\n",
      "Metrics for Fold 5:\n",
      "\n",
      "Median_meltLength:\n",
      "R² Score: 0.3744\n",
      "RMSE: 3.5856\n",
      "MAE: 2.6887\n",
      "\n",
      "meanNumPixels15:\n",
      "R² Score: 0.4347\n",
      "RMSE: 714.2639\n",
      "MAE: 570.5818\n",
      "\n",
      "Top 5 most important features for fold 5:\n",
      "1. d_col: 1140.7819\n",
      "2. d_over_z: 955.3586\n",
      "3. Z*d_over*d_col: 830.6110\n",
      "4. Z*d_col*d_over_z: 719.3218\n",
      "5. Z*d_edge*d_col: 596.1847\n",
      "\n",
      "Average metrics across all folds:\n",
      "\n",
      "Median_meltLength:\n",
      "Average R² Score: -0.2201 ± 1.0793\n",
      "Average RMSE: 4.6209 ± 1.7616\n",
      "Average MAE: 3.1149 ± 0.7181\n",
      "\n",
      "meanNumPixels15:\n",
      "Average R² Score: 0.4111 ± 0.0341\n",
      "Average RMSE: 706.5579 ± 10.2791\n",
      "Average MAE: 562.0385 ± 9.8618\n",
      "\n",
      "Average feature importance across folds:\n",
      "\n",
      "Top 10 most important features (averaged across folds):\n",
      "d_over_z: 870.5216 ± 50.2294\n",
      "d_col: 778.3505 ± 192.4046\n",
      "Z*d_col*d_over_z: 597.6386 ± 120.5919\n",
      "Z*d_over*d_col: 554.5304 ± 192.2589\n",
      "d_col*d_over_z: 367.9400 ± 59.2021\n",
      "d_edge: 329.0627 ± 104.7538\n",
      "d_over²: 326.2148 ± 42.7122\n",
      "Wall: 310.4500 ± 140.1411\n",
      "Z*d_edge*d_over: 291.8614 ± 17.8184\n",
      "Z*d_col: 288.3690 ± 128.2359\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['Power', 'Z', 'Y_Start', 'Wall', 'd_edge', 'd_over', 'd_col', 'd_over_z']\n",
    "\n",
    "model = PolynomialRegressor(input_dim=8, output_dim=2)\n",
    "\n",
    "metrics, importance = run_cross_validation(\n",
    "    model=model,\n",
    "    fold_indices=fold_indices,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    num_epochs=1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c31a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
